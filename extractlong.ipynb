{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract text from a long file video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1: import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install SpeechRecognition \n",
    "#pip install moviepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import speech_recognition as sr \n",
    "import moviepy.editor as mp\n",
    "from moviepy.video.io.ffmpeg_tools import ffmpeg_extract_subclip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2: Cut video file into chunks of 1 minute and convert each chunk into textÂ format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The video is 3120 seconds\n",
      "[0, 60, 120, 180, 240, 300, 360, 420, 480, 540, 600, 660, 720, 780, 840, 900, 960, 1020, 1080, 1140, 1200, 1260, 1320, 1380, 1440, 1500, 1560, 1620, 1680, 1740, 1800, 1860, 1920, 1980, 2040, 2100, 2160, 2220, 2280, 2340, 2400, 2460, 2520, 2580, 2640, 2700, 2760, 2820, 2880, 2940, 3000, 3060, 3120]\n"
     ]
    }
   ],
   "source": [
    "#1 hour and 26 minutes\n",
    "num_seconds_video= 52*60\n",
    "print(\"The video is {} seconds\".format(num_seconds_video))\n",
    "l=list(range(0,num_seconds_video+1,60))\n",
    "print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in converted/converted1.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in converted/converted2.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in converted/converted3.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in converted/converted4.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in converted/converted5.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in converted/converted6.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in converted/converted7.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in converted/converted8.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in converted/converted9.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in converted/converted10.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in converted/converted11.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in converted/converted12.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in converted/converted13.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in converted/converted14.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in converted/converted15.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in converted/converted16.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in converted/converted17.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in converted/converted18.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in converted/converted19.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in converted/converted20.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in converted/converted21.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in converted/converted22.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in converted/converted23.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in converted/converted24.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in converted/converted25.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in converted/converted26.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in converted/converted27.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in converted/converted28.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in converted/converted29.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in converted/converted30.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in converted/converted31.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in converted/converted32.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in converted/converted33.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in converted/converted34.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in converted/converted35.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in converted/converted36.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in converted/converted37.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in converted/converted38.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in converted/converted39.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in converted/converted40.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in converted/converted41.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in converted/converted42.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in converted/converted43.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in converted/converted44.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in converted/converted45.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in converted/converted46.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in converted/converted47.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in converted/converted48.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in converted/converted49.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in converted/converted50.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in converted/converted51.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in converted/converted52.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n"
     ]
    }
   ],
   "source": [
    "diz={}\n",
    "for i in range(len(l)-1):\n",
    "    ffmpeg_extract_subclip(\"videorl.mp4\", l[i], l[i+1], targetname=\"chunks/cut{}.mp4\".format(i+1))\n",
    "    clip = mp.VideoFileClip(r\"chunks/cut{}.mp4\".format(i+1)) \n",
    "    clip.audio.write_audiofile(r\"converted/converted{}.wav\".format(i+1))\n",
    "    r = sr.Recognizer()\n",
    "    audio = sr.AudioFile(\"converted/converted{}.wav\".format(i+1))\n",
    "    with audio as source:\n",
    "      r.adjust_for_ambient_noise(source)  \n",
    "      audio_file = r.record(source)\n",
    "    result = r.recognize_google(audio_file)\n",
    "    diz['chunk{}'.format(i+1)]=result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"so as I was saying today we will start with the final big framework major framework for machine learning which is rainforest meant learning and then in particular we will explore by the end of the class and also in the next class how to use deep learning in combination with reinforcement learning because of course reinforcement learning by itself it's not a topic that is tightly linked to declare ok it's more like a machine learning alright control theory topic in Factor enforcement learning theory has lot to do with dynamical system theory and control system Theory however we can exploit deep learning as a block in an enforcement agent to speed it up and to improve the quality of the learning Ajax based so today we will start by motivating why do we need to\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diz['chunk1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'chunk1': \"great so we are ready to start with this new class so today we will cover the final topic in the unsupervised learning framework which is generative adversarial networks I think this is a very exciting and cannot very new and on development topic which was defined by llamacorn which is one of the leaders in deep learning he was like the one of the inventors of convolutional networks he was the chief for a a Facebook AI research so he's like he knows about the history of the planning and the most recent advances and defined as one of the most exciting and insightful ideas that happened in the past few years in deep learning so this is just to emphasise how interesting and potential is this topic to help it\", 'chunk2': 'so we start by discussing an issue that happens when we use mean squared error as a loss function and usually we do this with the DPD network through the out and coldest relational out and code us we are always trying to reconstruct the input for example an image and then we take something like the Euclidean distance or their kl-divergence we try to make measure how well we reconstruct the data by measuring the distance between these two objects the problem with this approach is that if we only miss a few pixels like in this case the white object in the centre of the image The Miz carraro is not gonna change that much so the most career mode is going to emphasise that say the overall shape of the image that you are producing', 'chunk3': \"missing a few pixels out of thousands of pixels our loss is not going to be very affected by this however as you see in the image sometimes it's more changes are Critical so in the reconstructed image in this case you can imagine this is not a construction made by a bus station allowed to go there or by some other unsupervised deep learning models totally losing the semantics of the data in this case there's a robotic hand trying to pick up a small object if you're not creating the object in the scene the same totally changes its meaning and it is a very important issue so we should somehow be able to define the salience of each pixel in this case this that's a 9 pixels here are extremely saying\", 'chunk4': \"where to other pixels maybe on their top of the major on the Bible so if he needs to the contract this portion of the input here maybe it's not a big deal is not already serious problem however if we miss these pixels hear that we totally changed the meaning of the picture so we should be able to define how important every single input is to define the global meaning of the of the factor that say there is no easy fix it to this issue that have been proposals like Maps or to try to describe the scene at the more high-level stage or other approaches however today we will discuss a very smart idea this is that of adversarial games it has a little bit to do with adversarial attacks but not very much so don't be confused with the\", 'chunk5': \"ideas of adversarial examples that we have discussed previously when we were talking about supervisor classifiers so here we are concerned with an adversarial game which means that we have two players in a game that are trying to compete against each other so the object of the generative model so we're still focusing on unsupervised learning we want to build a generative model of the data is now to full a supervised classifier this is the key idea we are not trying to minimise the reconstruction Lost That professional we are not trying to maximize the likelihood over the data distribution itself we are trying to fool another I got it done which is a supervisor got either so we're combining unsupervised generation\", 'chunk6': \"supervised classification of the dataset in this case any single bit of information can be used for because in this previous case if you're classifier for example as to tell you if the image contains a ball like a leader object that needs to be picked up it can immediately see the difference between these two packets even if the difference is only in 9 pixels a classifier can exploit information in one single pixel if that is used full from the classification task so this is the general architecture for a generative adversarial network then we will be a little bit more into the details but there are many variations of this architecture so let me try with the general started with a general discussion of the main ideas so we have our training data as always so this is for example of distribution of\", 'chunk7': 'we want to be able to model these distribution using unsupervised we want to be the generative model of these data distribution so in the past glasses we have seen we can turn on out in cold air or was on machine or a deep belief network on these data to try to reconstruct it to reproduce it in this case however this is our unsupervised generative model we call it the generator sometimes we just call it G and the generator is starting from a latent back door which can be just around the back door in the most general case we sample around them back door of size and so we have a certain number of elements in his laptop and we generate in a single forward pass an image here so the generator is just', 'chunk8': 'random vector and produce something some patterns of activation over its output layer and then we have another piece in the Architecture which is the discriminator which is a classifier so now this is a supervisor even whose task is to classify fake images vs Real images so these discrimination discriminator module is taking randomly either one image from the data itself in that case the answer it should be real so this is a binary classifier or it is taking an image produced by the generator and in that case the output should be fake so we produce 0 if the image if the discriminated believes the image is coming from the data or 1 if the discriminative thinks that the data is coming from the generator', 'chunk9': 'now that many days in this architecture for example the late and back door I just tell you that it can be around just a sample from distribution from a uniform distribution that these serves the purpose of the Leyton called in and out encoder like number relational out encoder that is used to sample the data so this is very similar and they will be fine the latest code we will see later results in different kinds of gun models so there are some details about the late and called that we will explore later for the moment just think that disease sample from a random now I think that it is nice to think about these adverts are games as a game between counter Fighters and police so', 'chunk10': \"is constituted by these guys here which I like trying to do something illegal they are trying to produce some money some piece of paper that look like euros or dollars ok so the goal of the generators is to make sure that they earn money is as similar as possible to the real money so that the police which is the discriminate or cannot tell the difference of course if you're generative model is very bad like you produce just that one hopefully or a very 110 piece of paper $10 the police will immediately see that this is a fake image and you will be arrested ok however as you improve in your process of generating higher and higher quality fake money then the police will struggle a little bit\", 'chunk11': 'is that by the end of training your generator is so good that the police cannot tell the difference between the fake and Rihanna so this is the final so now we go to the Quorum generative adversarial networks so in this general setting the generator and the discriminator can be any I got either we can generate the data using a mixture of gaussians using k-means algorithm using a hidden Markov model whatever the discriminator can be our support vector machine but whatever you want however in generative adversarial networks we want to have the generator and the discriminator to be deep in your for the generator is another network that can take apart of random noise and then generate the data in high-dimensional space and you can use', 'chunk12': \"play.com additional you can be fine and pulling and pulling their yes you can have a more complicated sequential model you can really have inception models whatever you you need to make this complex enough and similarly the discriminator since we are producing high-level high-dimensional images in principle should be a convolutional neural network with arbitrary complexity producing a binary classification in this case we can train the entire system using back-propagation so this is the Kia dealer in gas we have another sorry again and since years in the game at both neural networks we can't rain all the ways of the system using back-propagation in particular the weights of the generative network or update updated to increase\", 'chunk13': \"we want to make sure that we produce fake images of such high quality that they cannot be discriminated and so the classification error will be very high however that discriminator wants to decrease this classification so these two modules are competing against each other I think this is really as Martin so I don't know why people didn't think about this before actually there were some similar ideas in the literature but it was not really working that well in practice but if you think about this this is raining quite straightforward you can implement it with pytorch without mine is Talos so now some technical details the learning Dynamics is actually based on game Theory so maybe some of you are taking the game Theory course or have already taken the course basically learning in this case is a minimax\", 'chunk14': \"sometimes record this 0 Sam games what does it mean somebody will win some somebody will lose there's no compromise that you will not co-operate agents that is a competitive game so the value function is maximized by one agent and minimise by the other so you can see this is the function for a gun the generator is trying to minimise this I mean to find the minimum minimal Model T the discriminator is trying to maximize it so he is our I don't function value from function you can see that it is composed by two-thirds to expectations actually because we can frame this as a maximum-likelihood problem however the maximum likelihood is not only related to the data itself which is this first time here\", 'chunk15': 'indeed of x4d is the discriminator G is the generator DX represents the probability that accent is coming from the true data distribution from our training set rather than be a fake pattern generated from my g so the discriminator wants to maximize the probability of assigning the correct label to axis if axis samples from the data and this is the first term and then also the discriminator wants to assign the correct label to fake data which is generated by g starting from the latest call to set so the discriminator is applied to the fake data and so we want to minimise maximise also this probability which is the generator network wants to minimise', 'chunk16': \"for the probability that assigns the correct label 2G and you'll see that g is only concerned by Disturbed here because this is the only turn LG has a role in this case this time here is totally due to the discriminator 4G cannot do anything about that so the discriminatory Panthers the connection weights for the discriminator network are updated by ascending the stochastic gradient because we want to maximize this function so we say look at the gradient because usually we are processing the data mini batteries or one back one chapter at the time so I'm is a number of patterns in the mini match or in the Chinese at and we are just trying to ascend the gradient of this quantity here\", 'chunk17': \"both the image sample from the data distribution and the one generated by g we are using logarithms as always because it's easier to work with the log likelihood likelihood and then the generator parameters are updated by the Sandy the graveyard because we want to minimise this point so this is the quantity that g is trying to minimise and you can see that it's only depends on this here in the value function so I'll let you reason about this for a few seconds so I hope you can see why these two models of the system are coming\", 'chunk18': \"trying to maximize opposite factions ok so there might be issues of convergence in this kind of game so if you have taken the game Theory class or if you're not something about game Theory you know that ideally we should converge to what is God and Nash equilibrium where the two agents are some why some way at the equilibrium point and the game can be considered so it's not easy to certify to make it sure that we terminate at equitable state but we have some assumptions and under some constraints we can reach such saddle point so the goal in this particular case is to achieve equilibrium point when the generator is producing data of such high quality that it is indistinguishable from the training data ok in that case if we are producing some\", 'chunk19': \"alternator the discriminator will not be able to solve it won't ask and it will just around me randomly gas true or fake by just Rowena coming so random classifier means that the discriminator has won the game requires a non-trivial synchronisation between the two agents playing the game because if we start as you can see from this image here this is the discriminatory cost so the lower the better ok and this is the generator cost so what is happening usually at the beginning of training is the generator has random connections so if you start from a random latent called zet0r you generate some data that is going to be quite easy for the discriminator to tell if the data is fake\", 'chunk20': \"because we have just comparing images with random pixels so at the beginning the discriminatory likely wing the game it was started to decrease and increase his loss ok and the generator will struggle because it's going to be hard to move away from this initial condition however at some point the generator starts to produce some more sensible distribution over the pixels to generate images of better quality and showing from that moment the generator loss we started the please and the discriminatory lost will start to get worse again so as the generator becomes better and better in generating high-quality patterns the discriminator will become Walsall and Watson until by the end of the discriminator will not be able to solve the classification problem so this is the goal\", 'chunk21': \"achieve after convergence however if we don't synchronise this it may happen that the discriminator lot with continue to go down and the generator will not be able to get the signal to generate some new values for the pixels because if the gradient disappears because the discriminator is always correct then you don't have let me signals to change the connection always so again if you're gonna to implement again I think overall can be quite easy because the Architecture is not very complicated but then you need to carefully synchronise the two modules and dizzy is a bit more thank you so in theory gansa quite easy to implement because the ideas were the same poor in practice it's quite complicated to have a gun working as you wish so I think there are 96 and let's say\", 'chunk22': 'shows that you can find online to synchronise these two models for example One idea is that you do more steps of learning in the generator rather than the discriminator so you do maybe you process 10 images with generator you generate and samples you try to learn by keeping the discriminator fix it and after that you do one update ocean the discriminator so you have two Loops in the algorithm and one loop is optimising the generator more frequently than the discriminator so they are nice property of Gants is that after you have learnt so after training has finished we can sample from the generative model in an extremely efficient way we just start from Random noise I will do 141 pass to generate the data this is something that is quite complicated to do', 'chunk23': 'usually we need to use approximate inference my thoughts sampling but if not influence in this case this is just a field full of pass so this is a very efficient way for generating you later another interesting property is that differently from variational autoencoders in this kind of Gants we do not need at this very accessible and colder so if you remember the vice architecture you cannot start really from random-noise I mean after training you could but in principle you also need an encoder here which is starting from sun data generating the code and then something from the cold so you have another portion here in the Architecture ok so you start from the data you generate somebody construction you sample reconstruction and then you back propagate so you need a differential code', 'chunk24': \"we start from scratch from the call we don't need any colder so we don't need to pass the gradient through this Factor and so sad can be a discrete cold and we will see for example in infogerance or other kinds of architectures this can be an advantage because we can try to factorise the late and cold more efficiently so this is an important distinction between but doesn't Gants in gangs we can use discrete codes now if we look at some examples you can see that if the ground truth is something like this if we use mean squared error in most of the cases we neglect final going in the details from the image because adding these pixels to the final image is not really changing\", 'chunk25': \"it's much so I got you those based on squad battles usually neglect these details however for a gang it is very important to include these details otherwise the discriminator will easily tell you that this is a fake image because we know that heads with have here's so if you generate a head more than like these are had image without these detail even a very simple classifier can tell you that this is a fake and it is a real instead you Philippine the address please we have a much more final going in the structure in the pattern which is making classification quite challenging it so this is why if you have to generate data like you want to use gas because these are\", 'chunk26': 'allows you to achieve this level of quality and precision in the pixels that are generated so these are some samples generated by a gun and you can really see that they look like but your faces they have no artifacts they are have a very high quality and we can move randomly in the generating called to generate variations of these faces so I think this is quite impressive so just to make it clear gas have been created last 6 years ago that the paper was in 2014 so at the beginning the quality of the generated samples was not that high now in the last couple of years the quality of the generation has really improved a lot up to the point of the receipt in the interviews lights open issues that', 'chunk27': \"implementing again one of the most evident one is called the mode collapse so the idea is that if you have a bimodal distribution and you want to capture the business solution by using a gang you may and up in a point where the gun is only approximate one mode of the distribution but that's enough for winning the game because you just generate samples very similar to the first moon you totally neglect the 2nd note and the classifier will not be able to tell if you're generating by the examples of good examples because we are focusing on just one specific subtypes of your training later so let's take an example from the Ms you may just generate very high quality 7th and once just to GTG but of of very high quality and the classifier the discriminator will not be able to tell\", 'chunk28': 'between these fake images and Ariel and digits however this is not capturing the entire complexity of the end list this is only for using on few mods in the distribution the mode for seven and the morning so this is a very critical issue of gas they are not guaranteed that they will try to cover all the complexity of the distribution they may just f*** you or one easy to generate more and still the game will finish because the classifier is not able to tell if the data generated from the generator is fake related to the data that is in the chemist so this is one of them most serious problems now I think them in the past few years they tried to sort this by adapting some more complicated methods so I think that you can find something in the literature that', 'chunk29': \"this is the second issue is can use a gan to learn some representations of the data so because we are starting from a random cold and we generate some activations in the output layer how is it possible to if I don't show an image to a gan a real image to create an internal representation what we do without on coldest we start with an image with compressive we are encoded into the late and space and that will constitute an internal representation with a tad extracted some features that I can use to perform some other calculations to Georgia similarities between images to train a classifier on top of that representation I can do many things if I have some data and I cannot represent it in a feature space with guys this is not immediate because gas can only generate that they cannot\", 'chunk30': 'so this may your should be what time shall limitation of gas of course in this case people have tried to fix it like we can look for the directions in the latent called that are making the sample generated by the Gang more similar to the target image to the selected input that I want to in court so I can tell you ok this is a face that I want to encode tell me what is the latest cold of this face so you can start generating data with the gang in such a way that the generated image looks very similar to the target one and then you go back to the latest court that generated that gan image and you take it as a representation for your data so just had two issues that are warfare thinking about not let me mention some', 'chunk31': 'architectures now this is not a computer vision course is not that computer graphics course so I can really spend that maybe many classes just covering different architecture because they are producing a meal 10 or 100 architectures every month and each architectural is trying to fool maybe a particular issue related to revise learning so let me just thought using few of them and then I like it to you to explode the topic you can find plenty of resources online so the idea is that we can create what is colder conditional gun which is represented here the bottom sorry here at the centre by including some class information in the late and space so in the standard Ghana we have a latent back door we generated using the generator so Jesus is the fake image and the discriminatory staking either the axe or', 'chunk32': \"age and trying to buy makeup binary classification over these two classes now in a condition again besides the standard late and cold we are also providing some class information in the code ok so let's imagine we are trying to generate the Emily's digits besides around them back door I can also provide a bachata of 10 units representing the 10 different classes for each image and so if I activate the first element I'm trying to have the generator generating digits of 0 if I activate the second element in the sea code I'm trying to generate once and so on and so forth so I am trying to give some supervision that say to the generating process in order to be able to specify from which class I want the data\", 'chunk33': \"of course I want to also give this information to the discriminator because then the discriminator should also tell you is this image really coming from class 0 is this image really coming from class 1 and the destination also take advantage of this information so this is a potential way to extend the generator and be able to specify the specific class from which we want to generate the data so that we have trained again on imagenet image that contains faces cars trees buildings whatever concept we have and if I want to generate some image from imagenet I can just take a random vector and then I will get maybe a tree or a face or a car but if I want a picture of a car how's it possible I need to also be able to provide class information to the generator for conditional Gans R1\", 'chunk34': \"little Ouse this then we have oxygen reclassify against we're still we have class information in the latest code ok but we also include the class information as output for the classifier so it is basically changing from a binary classification real or fake to a binary + 90 + classification we want to tell if the image is real or fake and also from which class the image was sampled so with junior into a semi-supervised problem so now the classifier can try to perform at finally died in the classification over the data that has been generated when does a very famous architecture which is called the infection if I remember correctly was created by open AI\", 'chunk35': \"which stands for information maximizing gun and I don't want to go into the details of this so basically we are adding regularizer to the loss function which is trying to maximize the moon 20 formation between the latest cold and the distribution generated by the generator ok so it needs a little bit of mass to be properly the fine but you can find all the details in the paper I guess it's a paper from 2006 but what is the main idea that we want to add besides some class information like an incision we also want to add some semantics to the back door inside so we want to assign some meaning to the features that we want to sample from so for example when you are generating and Miss images of course if you manipulate the digit class you\", 'chunk36': \"digits ok so if we very see one which is the code for the digit we can choose with which digit we want to sample from the ground and is something that you can achieve with us stand-up conditional however more than this we can also say ok I want to have a random variable representing the angle of the digit another random variable should represent the rotation another one show represent the width of the other side so if I only change the random variable in the cold I'm expecting to see changes selectively specific for this feature so you can see it here if we generate this pattern from the generator and then we vary the sea to feature in the Intel microcode we can rotate\", 'chunk37': \"ok so you generator different azimuth today produce image or if you change another variable yet another variable from the latest code you can change the width of the stroke so you start with very thing digits and you move up to six digits so this is very related to the idea of bitter when you're out and cold as well we are trying to factorise the late and cold in such a way that we can money too late selectively one bit India not on B21 variable in the latest code to selectively change only one property in the image now it's very promising as an architectural so in forgotten and beta by Eternal out encoders are considered like state-of-the-art architecture for implementing this kind of process but still is quite challenging to achieve this result\", 'chunk38': \"example one of the goals of FIFA games to say I want to generate an image and by playing with one neurone with one feature in the latest cold I want to change the colour of the face for example or the colour of the Apes or the length of the years so this is quite complicated to achieve in practice so with some toy models like the M&S you can do something like this with more complicated distributions this is still being developed so this is one example of faces generated by infection and I really think we can spend a few moments and looking at this fishing this I wish the entire desktop so it's going to be much more\", 'chunk39': \"so we don't need forgotten you can play and try to only change the hair colour for example of the images that so you tried to maintain everything else the same and you only play with that one feature or you play with the rotation because you are acting on another latent unit or will change the face over so the size how fat is basically the picture of the face that you are generating or the week how how long it is hairstyle go from let's left side to right side so you see we have trying to smoothly change the pixels by\", 'chunk40': \"manipulating one feature at the time I think this is quite impressive because I mean these distributions are much more complicated at the feminist distributions so in this case you'll see we have high quality images with many pixels many colours final Graham features for this is really something impressive in my opinion we can change the mouth from open to closed and song and so forth so I let you look at the video maybe later so still you can see that the samples are not very careful they have some Lidl that's a imperfections that a human high can tell the classifier was not able\", 'chunk41': \"tell if this was a fake or real image now another architekten which is getting very popular is called the buses tain gun the fury is a bit complicated I tried to look at the proofs in the paper and it's cannot very highly mathematics however lady is very simple so I would just try to tell you about the idea so the discriminator rather than being a binary classifier or a multi-class multiclass classifier is replaced by a critic just basically a classifier that is trying to call the quality of the image so we're not just trying to say ok this is fake this is real this is fake we can also say this is fake but not too much or yet this is totally fake or yeah this is real but maybe he has some imperfections so\", 'chunk42': \"regression task to have a much finer grain control over the output of the classifier and basically with this change you really have training much more stable stable and fast so if you're going to implement again I encourage you to use our like it's a soft classification like regression task rather than vacation because this is improved convergence then we have a cycle young which is something that get some popularity in the media basically in this case we are trying to learn image to image translation we have starting from one image and we want to transform it into another image with some control the properties in ages that you can do this by having paired training\", 'chunk43': \"I want to transform a picture into a cartoon picture so I take a picture of you and I tried to convert it into a an image that resembles something that has been painted if I have a large dataset with pictures and the corresponding cartoon pictures then you can do this you can find encoder and decoder model that is like I hadn't noising out and called that you take an image of a kind and you mop it into another one but if you don't have paying images this is a quite challenging task and would cycle counts you can sort this problem without the need of paying the timing images and I will show you an example in the next slide about this and finally steigen this is quite complicated I didn't have the time to look into the final going details of this architecture which was published last year so it's very new architecture\", 'chunk44': 'playing a little bit with the generator is not just a convolutional neural network it is a more complicated structure where you can have the code in for being a different layers of the hierarchy so the generator is significantly changed now but the idea is that with tigers you can produce amazing images so now I will show you also the level of details that you can achieve with this kind of generator so this was sponsored by NVIDIA we have some intro and so dizzy is the quality that you can have as the output generated by again so these are really images that looks like a real pictures I cannot tell the difference not the explain', 'chunk45': \"The Beatles I think that what is impressive here is that you can have different levels of features you can go from low-level features that are the final like the colour the shade and this kind over low-level graphical features to high-level features like the length of the female vs male face and you can play with all of this so you can fix the Corsa style and just play with the middle Styles and you will get variations on this OK for example in this case you're trying to have images to generate an image that takes some features from all these different levels of complexity\", 'chunk46': \"ok I like you play with these a little bit so some examples from the cycle gan you can transform a horse into a Zebra by doing this kind of transferring of some features from one image to the other so this is what we meant when we were talking about image pay as if you have this kind of information in your train is that you may have these two images using I'm out and colder if you don't have paid images than you can use a cycle yeah I'm busy I think quite impressive also because you can make it like in a movie and you can make it in single frames of a movie and so you can just take a video\", 'chunk47': \"is that you want to replace the texture of the skin of the horse with that on a zebra or this is maybe the most time as application I don't know if some of you have found is online before you can start with a fake picture and change it to some target painting style so I want this picture to be transformed into a money style painting or a bungalow style painting or a season or some Asian schools of painting and the Gang will try to match these features the features that are defining the money paintings into your Image so this was a very popular in the class because you can upload your own picture of your face and have a self-portrait made by this is a picture generated by the stag and that we have seen before in the video\", 'chunk48': \"website that god this person does not exist we're basically every time you refresh the page of the website it generates a new image and these images look like a real person but no existing person they are totally fake images generated by a gun and this was like producing awesome Media rumours because they're 190 Facebook profiles fake Facebook profiles with as a profile pictures something generated by Gant so you can imagine how this can be applied to like fake news and generating content online you can do something similar with artwork so this gun was trying on a large dataset of artwork and now it is able to produce some possible paintings or pieces over the art and every time\", 'chunk49': \"hey Joe we get a 35 piece of artwork now busy I think mostly abstract painting so sometimes they look nice sometimes they look ugly but this is also for true abstract paintings I think so it's a bit subjective but I think that overall they are quite well formed and they appear quite plausible to my eye at least now I will just come from the class I think we have sometimes full questions maybe later with an example of application in the ICT setting which I think it's very cool and easy is also sponsored by mbb so they do that now we're doing all these rooms and therefore meetings and we are really almost like it's a congestion in the telecommunication systems because\", 'chunk50': \"high levels of traffic's that are generated by video conferencing systems so they do that if you have a webcam that is capturing your face you can just take a snapshot of that and that will be a keyframe which is sent to the receiver and that you have some basic I got it that is trying to do some keypoint extraction so deserve basic computer vision algorithms that can extract some key points from your face and then you send through the internet only the key points and the keyframe you don't need to send the entire video you only see this keyframe the coupons for the key points of course I'm moving as you change the expression as you open your mouth you close your eyes and then the Gang is basically applying this transformation in the key points to the keyframe so it is producing the output without them\", 'chunk51': 'send the video sex assualt you can have a AI video compression two teeth that really occupies the same or even lower bandwidth compared to standard encoding schemes but you have a much higher quality in your video call so I think this is also quite interesting to see so you have somebody talking in the webcam this is just how much you can compress it compared to h264 you see the numbers but I maybe you can look at it later let me see if we have some you see so this is the true video and this is the video generator', 'chunk52': \"I just taking a single frame which is a steel frame and then some key points and then you apply the movement of the key points to the frame and you can regenerate something that looks pretty much like the speaker so you can really improve of orders of magnitude of the quality of video calls without sucking up with the bandwidth that you have at your disposal so I think it is a very cool application of gas and Moira yes to come I mean this technology is so new that you can barely imagine all the ways that you can apply this powerful I already done so as reading you have the original paper published by Goodfellow and colleagues and I think that you should really try to read it because it's well waiting and it explains for the basic details of the van so I think it's nice Serena\"}\n"
     ]
    }
   ],
   "source": [
    "print(diz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3: Export results into a TextÂ document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_chunks=[diz['chunk{}'.format(i+1)] for i in range(len(diz))]\n",
    "text='\\n'.join(l_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'so as I was saying today we wi'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ready!\n"
     ]
    }
   ],
   "source": [
    "with open('recognized.txt',mode ='w') as file: \n",
    "   file.write(\"Recognized Speech:\") \n",
    "   file.write(\"\\n\") \n",
    "   file.write(text) \n",
    "   print(\"ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
