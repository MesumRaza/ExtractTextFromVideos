{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract text from a long file video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1: import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install SpeechRecognition moviepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import speech_recognition as sr \n",
    "import moviepy.editor as mp\n",
    "from moviepy.video.io.ffmpeg_tools import ffmpeg_extract_subclip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The video is 5160 seconds\n",
      "[0, 60, 120, 180, 240, 300, 360, 420, 480, 540, 600, 660, 720, 780, 840, 900, 960, 1020, 1080, 1140, 1200, 1260, 1320, 1380, 1440, 1500, 1560, 1620, 1680, 1740, 1800, 1860, 1920, 1980, 2040, 2100, 2160, 2220, 2280, 2340, 2400, 2460, 2520, 2580, 2640, 2700, 2760, 2820, 2880, 2940, 3000, 3060, 3120, 3180, 3240, 3300, 3360, 3420, 3480, 3540, 3600, 3660, 3720, 3780, 3840, 3900, 3960, 4020, 4080, 4140, 4200, 4260, 4320, 4380, 4440, 4500, 4560, 4620, 4680, 4740, 4800, 4860, 4920, 4980, 5040, 5100, 5160]\n"
     ]
    }
   ],
   "source": [
    "#1 hour and 26 minutes\n",
    "num_seconds_video= 60*60+26*60\n",
    "print(\"The video is {} seconds\".format(num_seconds_video))\n",
    "l=list(range(0,num_seconds_video+1,60))\n",
    "print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in converted/converted1.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in converted/converted2.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in converted/converted3.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in converted/converted4.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in converted/converted5.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in converted/converted6.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in converted/converted7.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in converted/converted8.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in converted/converted9.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in converted/converted10.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in converted/converted11.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in converted/converted12.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in converted/converted13.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in converted/converted14.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in converted/converted15.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in converted/converted16.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in converted/converted17.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in converted/converted18.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in converted/converted19.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in converted/converted20.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in converted/converted21.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in converted/converted22.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in converted/converted23.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in converted/converted24.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in converted/converted25.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in converted/converted26.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in converted/converted27.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in converted/converted28.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in converted/converted29.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in converted/converted30.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in converted/converted31.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in converted/converted32.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in converted/converted33.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in converted/converted34.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in converted/converted35.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in converted/converted36.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in converted/converted37.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in converted/converted38.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in converted/converted39.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in converted/converted40.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in converted/converted41.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in converted/converted42.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in converted/converted43.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in converted/converted44.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in converted/converted45.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in converted/converted46.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in converted/converted47.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in converted/converted48.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in converted/converted49.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in converted/converted50.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in converted/converted51.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in converted/converted52.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in converted/converted53.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in converted/converted54.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in converted/converted55.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in converted/converted56.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in converted/converted57.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in converted/converted58.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in converted/converted59.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in converted/converted60.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in converted/converted61.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in converted/converted62.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in converted/converted63.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in converted/converted64.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in converted/converted65.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in converted/converted66.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in converted/converted67.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in converted/converted68.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in converted/converted69.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in converted/converted70.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in converted/converted71.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in converted/converted72.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in converted/converted73.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in converted/converted74.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in converted/converted75.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in converted/converted76.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in converted/converted77.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in converted/converted78.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in converted/converted79.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in converted/converted80.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in converted/converted81.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in converted/converted82.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in converted/converted83.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in converted/converted84.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in converted/converted85.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in converted/converted86.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n"
     ]
    }
   ],
   "source": [
    "diz={}\n",
    "for i in range(len(l)-1):\n",
    "    ffmpeg_extract_subclip(\"videorl.mp4\", l[i], l[i+1], targetname=\"chunks/cut{}.mp4\".format(i+1))\n",
    "    clip = mp.VideoFileClip(r\"chunks/cut{}.mp4\".format(i+1)) \n",
    "    clip.audio.write_audiofile(r\"converted/converted{}.wav\".format(i+1))\n",
    "    r = sr.Recognizer()\n",
    "    audio = sr.AudioFile(\"converted/converted{}.wav\".format(i+1))\n",
    "    with audio as source:\n",
    "      audio_file = r.record(source)\n",
    "    result = r.recognize_google(audio_file)\n",
    "    diz['chunk{}'.format(i+1)]=result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"so as I was saying today we will start with the final big framework major framework for machine learning which is rainforest meant learning and then in particular we will explore by the end of the class and also in the next class how to use deep learning in combination with reinforcement learning because of course reinforcement learning by itself it's not a topic that is tightly linked to declare ok it's more like a machine learning alright control theory topic in Factor enforcement learning theory has lot to do with dynamical system theory and control system Theory however we can exploit deep learning as a block in an enforcement agent to speed it up and to improve the quality of the learning Ajax based so today we will start by motivating why do we need to\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diz['chunk1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'chunk1': \"so as I was saying today we will start with the final big framework major framework for machine learning which is rainforest meant learning and then in particular we will explore by the end of the class and also in the next class how to use deep learning in combination with reinforcement learning because of course reinforcement learning by itself it's not a topic that is tightly linked to declare ok it's more like a machine learning alright control theory topic in Factor enforcement learning theory has lot to do with dynamical system theory and control system Theory however we can exploit deep learning as a block in an enforcement agent to speed it up and to improve the quality of the learning Ajax based so today we will start by motivating why do we need to\", 'chunk2': 'learn by enforcement of any particular why we need to act to interact with the environment if you want to learn by the models of the word and if you want to build more efficient AI systems the challenge is that we are going to learn from trial and error which is quite demanding from a computational point of view because if we have an unlimited sample size we can really simple all the possible actions that we can taking the word that maybe we can converge sooner or later but if you have a limited time and limited computational resources learning from trial and error can be quite complicated I will just introduce the formalism of Markov decision processes which is the way we automatically set the enforcement loving problem so we will talk about the rewards policies value action', 'chunk3': \"snooze at the building blocks for the reinforcement learning problem I will show you the Ballymoney question then I will not go into the details about this because this is really topic for an ecosystem theories and dynamic programming but then I work I will cover the two most commonly used for reinforcement learning temporal difference learning and the extension to you later I will then discuss some major issues that arise which is the explanation that was exploitation dilemma and then I will also point out to some differences in without releasing off-policy my thoughts this is related to the behaviour policy that the agency's exploiting and then finally we will understand why you we may use neural networks in unfortunate let me problems because we can cast a portion of enforcement learning as a function approximation problem and so we can use a DPD\", 'chunk4': \"approximate dysfunction and disease what lead us to the deep reinforcement learning set up so it's going to be any x class of course or as always we do a little break maybe before the final deep around topic to get questions and try to emphasize some critical points maybe so first I wanna just open a photos of about why do we need to act on the environment so we need to talk about Kawasaki which is a philosophical topic because it's not totally clear what does it mean to discover causes and effects from the world but for sure what we know and what you should know from your statistics classes is that if you observe something for example you are putting your efforts to move something and you see that it is moving forward you say ok because I'm moving the object I'm acting on the\", 'chunk5': \"in a way that allows me to see some effects this is a correlation because we are not sure that the effect is Reading due to our home produce you ok so maybe the true colours for the effect that the object was moving is due to some other causes ok so we have to split between correlation and learning which is only based on observation and Calorie which is based on something more complicated so correlation does not imply causation raises a problem because if we only have serve to evidence it is hard to say which is the cause and which is the effect so there are good reasons and now again this is a bit off-topic but I think it's interesting to discuss that we need to to ask\", 'chunk6': \"manipulate the environment if you want to move from correlations to conversation if you want to discover car relationships we need to do something more than just observing the sensory everything's ok so this is one of my favourite example imagine you are very strong correlation between two events like the song which is raising up in the morning and the Cock which is singing every time in the morning so we have a cock crowing and the sun Racing app and this to have inside strongly correlated they are cured with a very systematicity and actually if you look at the time for a sequence may be the cock is crowing before the song is raising up so enable server may be tempted to the news that the\", 'chunk7': \"knowing is the colours of the Sun Racing App because that Evans is always happening before and is always perfectly correlated with the song it's so we might say that the cock is causing the sun to raise and of course you know this is not the case maybe it's my birthday so we may say that the sun is causing some lights to appear in the sky and and caugheys receiving this difference in the light and it's starting to grow and that the sun is actually raising up how can we discover these causal structure how can we tell that it is the sound housing the Cock singing we need to intervene need to act on the on the environment so we have two options we can try to stop the song for example we freeze the song and the next morning we see if the copy is\", 'chunk8': \"OK that is are quite complicated interaction to make or vice versa we can intervene on the car so we can for example tie the beat of the animal and we will see that even if the animal is not allowed to sing the next day the song with Steel this way we understood that the copy was not because of the salary but we needed to interview to do something in the process another example is this we have a kid sitting in the plane and she's saying I wish they didn't turn on that seatbelt sign so much every time they do it gets back so the naive cows and model is that because the seatbelt sign is being turned on before the bombs the the cause and then the box is the effect\", 'chunk9': \"we know the truth as a model is probably this one we have a storm approaching the pilot will see this between turn on the seatbelt sign and after a while we perceive the box so there's a hidden cause that the kid is not able to realise my just observing the environment I can go on with many other examples like their correlation between the chocolate from Samsung and the number of Nobel laureates medium population and you can see the reason for example is a heavy chocolate consumption country and also lot of noble for you may say ok maybe I should eat more chocolate to win more than normal prices of course these are stories correlation probably probably chocolate consumption is related to the wealth of a country because maybe some expensive type of food and the web design related to the educational system that you may have have\", 'chunk10': 'as long as a 4.4 short chocolate from Sanford he is not the cause of getting the price and then finally it is probably the most famous example we may ask if that smoking can cause lung cancer and so in the 50s or maybe 40s there were many people trying to start the trial against Philip Morris because they weather assuming that smoked worse was causing lung cancer and they wanted to be paid back for this disease so this was the causal model that the customer customers who are putting against Philip Morris Corfield Morris tired the most famous statistician in of the time Sarah Fisher who defended Philip Morris in the card and Philip Morris', 'chunk11': 'wallet the case was able to demonstrate that there was no evidence that there was no evidence in favour of these cows are model because maybe in fact you have cancer which is causing you some desire to smoke that was also possible as a model or maybe you have some genes which are present at birth which is causing the Cancer to appear which is Which are also causing your Desire of smoking for this at all possible cars are mothers and at that time there was no way to distinguish between 208 between different so Philip Morris was safe at the moment and of course he shot itself was strongly smokers for maybe he was trying to convince himself nowadays we have much better call Saul models of this process and we know that smoke can you', 'chunk12': \"cars land cancer what we have done a lot of studies for example of randomised trials where we have a population actually not a human population because that's not a tickly approve approved but you have a population of animals which is tested in such a way that in the control population you have the normal life for the animal in the clinical population will say you have animals is smoking or at least you tried to mimic the effect of smoke in the lungs and you will observe that you have a difference in the council probability between the two populations so the idea of having a randomised experiment is to try to solve this problem but the take-home message is that if you cannot intervene you cannot modify the environment you will never be able to discover that conversation you\", 'chunk13': \"only discover correlations so you can have a very powerful deep network you can say anything supervised or unsupervised it will only discover correlation Saturday if you want an AI system to discover cows and models the agent needs to be able to interact with the sensory environment so if you're interested in these photos of vehicle topic of causality I suggest you to take a look and read at work He has a very nice introductory lecture that you can find online about the the art and science of cause and effect but also his trying to push artificial intelligence agents towards learning more complicated words towards by interacting with the environment so I think this is one of the Frontiers of research\", 'chunk14': 'what is a bit of topical post just to introduce you to this idea that we need to interact to act on the environment to learn more powerful predictive models so reinforcement learning is heavily based on research on animal conditioning so people studying psychology will for sure know about this from previous courses so the way that animals learn has been studied using this reward based approach so in the classical conditioning paragons you have some stimuli that are provided to the animal and you for example try to associate one particular stimuli with the reward in such a way that the anyone we learn to predict that award based on this neutral stimuli', 'chunk15': 'so this is the famous Pavlov dog basically we know that if the dog will look at some food it will start to celebrate because the dog noses between get some food and then the dopamine serotonin with some celebration talk to you if you just drinkable the dog with not celebrate because the bell is a neutral stimulus however if you ring a bell every time you give some food to the dog after a while the dog will learn to associate these stimuli and then when you ring a bell the dog we start to celebrate so the dog Wheeler and Association between a neutral stimulus and potentially roundabout steamos like food so this is the idea of classical conditioning but in this case the dog is still a passive Observer it is discovering a correlation in a more Sophie', 'chunk16': \"Katie the set-up we have what is called the operant conditioning or the instrumental conditioning so basically the animal now needs to interact with the environment in order to select some response some action that needs to be taken and this was first introduced by Skinner in the last century so you can see Skinner was a bit more Devil Inside and he wanted to probably probably animals in more challenging settings so you have arrived which wants to receive some food sorry so there is a pallet dispenser which is giving some food to the rat however in order to get the food the Rock must do something complicated like personal ever accept a number of times or if we have a red light you have to press the lever or you have the green light you don't have to press it so you need to interact in a mo.com placate\", 'chunk17': \"with the stimulus environment or you may hear some sounds from the speaker so if you do choose the proper action you get the positive reward which is the food in some cases if you choose the wrong action you can even get the negative reward like an electric shock ok in that case that will be very careful and try to understand what is the best action to take and is where you can reshape very complex behaviour covid-19 on the story is a bit more complicated like if you give lot of negativity words that get stressed and it will not learn nothing more so it's a delicate balance between rewards and punishments but this is a motorcycle logical topical saying is that you try to establish the best way to interact with your environment in order to maximize the\", 'chunk18': \"Amy Myers the punishments and busy is there enforcement learning Saturday so in this case we have learning from trial and error at the beginning the rap that has no knowledge about the environment it will just see some light sleeping or give some noise from the speakers and he would not be able to press the lever in the correct moment but after some tires and some emeralds it with his Lowlander what are the actions that are actually giving a property worth for the fundamental problem alright fortunately is is which actions should I choose to perform in order to maximize my future awards and of course you can also minimise punishments so there are many complications to this basic setup the first one is that maybe some actions that I'm taking Max amaiz\", 'chunk19': \"immediate reward but then in the long term in the future they will be counterproductive so maybe now I can take that because I feel very sleepy and just want to sleep but I mean you will lose some information but will be asked to India exam and so at the moment you have somebody want because you feel recover than you don't you are not asleep again but during the exam you will realise that maybe that was not the good movie to take or rights of US Army actions may seem useless but in the long-term they will give you some important but it works so maybe now you are studying somebody challenging classes and it requires lot of effort and you don't see any feedback and reward but maybe in 5 years you will have a very well paid job because you have learnt something in your\", 'chunk20': \"glasses and so you will get your reward longer-term future so the possible solution to this complication is to learn to predict the long-term outcome of your actions this is a very challenging task to perform the environment is so maybe You are acting in a certain way and you're getting somebody words and the day after your acting in the same way that you are getting naughty work because it's not always the case but what you do is following but the same kind of the word in this case we may want to use a stochastic action policy so I'm not going to deterministically choose which action to take but I'm sampling the action according to Samsung ability but again this is quite complicated problem to solve\", 'chunk21': \"and then we have the same as exploration vs exploitation dilemma social we explore new and potentially more rewarding actions or should I stick with what I already know and I keep on doing the same things which I know that maybe we give a little bit more safe for you so for example should you go to stadia totally different topic who knows maybe you don't like the topic can you lose a lot of energy and you invest lot of effort because you have to wake up to go to the classes to spend the money and then you realise that was not interesting or maybe by standing a new topic you discover something great and it would be very rewarding but you don't know that in advance is so we may be lazy and try to just exploit what we already know or we may be very cool\", 'chunk22': \"explore something new so usually England Portsmouth love you a possible solution to this trade of is to start with a very high exploratory behaviour so like in simulated annealing with very high temperature we explore the environment we try many different things and then slowly we decrease the temperature and we try to convert to watch some good action plans actual actual policies and we keep on repeating the same actions we don't want explore no more and maybe this is something similar to what happens in biological systems if you go to a very old person to some elderly and you asked you are 70 years old do you want to go to China learning Chinese discover new kind of food learning new skills they say no I'm too old I just want to speak the language\", 'chunk23': \"before they like and easy for the places that I know this is like a more rigid behaviour that is maybe due to this necessity for exploiting rather than exploring at some point when you're around you are more willing to discover and try new things even though they may be contact productive so these are just some complications that needs to be tackled by reinforcement learning ages now let's try to formalize this problem and I would describe you the Markov decision process so this is the general setup for a reinforcement learning problem we have the agent and we have the environment and we haven't been loop of interaction so we have a set of environment state which recall asum this can be a very complicated state so in the most simple case the state\", 'chunk24': \"pizza binary state or is composed by a few bits in the most complicated cases we receive the state can be very high dimensional vector of may be continuous values and this is quite challenging to manage their we have a set of actions that can be performed at every time step in a game the action can be a set of discrete possibilities like her up down left right if you're trying to explore agreed word for example however the actions can also be in a container Base so for example and action can represent the force that from a given to your muscles to move your arm so your robotics for example we have lots of campinos action spaces which is making the selection of the actual very complicated then we have a set of environment observations\", 'chunk25': \"so what we can proceed from the environment and in fact the observation is basically the combination of the stage and the reward and now we have some rules that allowed to transition between different states so what is the probability of moving to state the plus one given that we are in the state at time T and we take a setting action and now we have other rules that are data mining the immediate reward which is can be it's a positive a scholar number or punishment which is a negative number associated with a certain transition and so the reward function is given us the reward at the next time step given the state we were before and the actual that we have chosen so this is the loop the agent select One action from the possible actions that can be performed\", 'chunk26': \"this action is causing some changes in the environment and so the environment will provide a new state and reward today agent and based on the information age and needs to select another action which will change the environment and that produce another word another state and we continue this look for that of course sometimes maybe the action is do nothing so we can also have this kind of the fake action that is just hold on I'm gauge and doesn't mean to do anything at that moment but that is also not sure you have to choose to do nothing or maybe some cases we will get no rewards because we are just in a nutri situation so that he was 0 or maybe the state will be the same of the previous date with one action\", 'chunk27': 'and the status not change so there are many possible tricky situations that complicates this interaction so at each time t agent receives an observation which is consisting in the current state st and the reward reward man u r t and then choose an action from the set of possible actions which is causing a transition to the max state with an associate reward RT plus one so disease basically the process the goal of the agent is to maximize the accumulated so this is what we want to achieve in supervised learning the goal was to minimise the classification or regression in unsupervised learning the reconstruction era or the way we match the timing distribution in fortnite learning the goal is to maximize the', 'chunk28': 'of all the rewards that we get so the reward can be positive or negative and we won the same to be as high as possible so you can see why we call this Markov decision process because we have the Markov property and by the indus loop basically if we know the current state we are kind of independent from the previous state because the action is choosing based only on the current state of the system however of course you can complicate this and made it possible for the agent to have an internal state like a dynamical memory like an lstm which can incorporate passed information because in the most common places where we have to select an action we also need to consider the trajectory of the states that leave to the current situation', 'chunk29': \"so how can we maximize the accumulated rewards so sometimes we called the accumulated rewards their Richard so if you will look in the shooting in Barton book they introduce this quantity g which is called the richer but actually most of the cases we have just talking about the reward so we mostly use are as available so the overall return is the sum of all the possible rewards from time 32 time capital T we have two keys here one is called the finance or eyes on Kay's will we just want to summary Wars up to a certain point is like saying I want to maximize my return for one game of chess I'm trying to learn to play chess and I want to win this only gay so after the game is over I got\", 'chunk30': \"however despite a difficult situation in most of the cases we have an infinite Horizon so I'm trying to maximize the rewards for my entire life not just for the chess game want to win all the chess games in my life but I want to win the chess game but also to eat some food to learn something from the class to get the feedback from my exam to etc etc ok so I want to maximize that work over the entire life of the agent so in that case what we need to do is to introduce a discount rate just got gamma because we want to maximize rewards forever so kegos from 02 infinite however we want to balance the images the immediate rewards and the very very long memory was so if gamma gamma is between 0 and 1\", 'chunk31': \"can be up to 1 actually in specifying how much we care about very long-term memory was so if gamma is equal to 1 the reward that we will receive after 1 billion steps will have the same weight of the reward that I'm receiving now or in few seconds and this is my be unwarranted because if somebody is telling you now you have to do something because in 10 years it would be good for you you said ok wait I don't know if I'm here him 10 years so sorry to tell me give me some feedback that I can exploit also today tomorrow in the next week maybe but don't go too much far away in time and give him my reward because I don't know in 50 years what could happen so if gammon is smaller than one you can see that we haven't discounting\", 'chunk32': 'the long-term future something that will happen in 10 years we have a smaller weight compared to something that we have a 1-minute from now so in the infinite arise ok we have tried to discount the rewards that will happen too far away in the future so we can fork used on the more immediate and we just do this by introducing this gamito here ^ t which is the timestamp however the fact that we may have delayed rewards makes learning very very complicated so imagine you are playing a task game the reward usually only is only provided at the very end of the game if you win the game plus one if you lose the game - 1 if the game is is', 'chunk33': \"draw 0 ok so we may have this rewarding scheme of plus 1 for women - 14 losing and they have many actions in the meantime maybe you have take your 40 actions and so if you want the game you may wonder which was the best action that I took Aldwych where the actions that led me to win the game and which was maybe but not very good actions so this is called the credit assignment problem if the reward is coming in the future it is very complicated to establish which was the action that was pushing myself towards the goal to watch the reward so you may introduce we will see in the masterclass some intermediate 3 was like if you capture the queen of your opponent I will give you 0.2 or because that's a good\", 'chunk34': \"maybe a power rock you get 0.1 so you can try to make the reward structure more fine-grained to encourage the development of some actions of some strategies but it is quite complicated problem that's so how can we try to solve these reward maximization problem first we need to define what is called the value function which is basically a function which is assigning some number some scholars to every state which can be in so it's basically the expected community return from that state we are talking about expectations because we are not sure about that it's a guess so for example I'm acting on the environment I'm playing some chess games and I see that on ABBA\", 'chunk35': \"it is good if my queen comes closer to the King of my of my address of my appointment so that might be something that suggests that that is a good stay to achieve I'm not yet winning the game but I'm approaching a state when the probability of winning is higher so we're talking about expectations and these can be averages over multiple games multiple episodes that we have us so we have collecting statistics to estimate what is the what is the value of every state so the trick that allows to tackle this problem computationally is that weekend recursively I'm park so we can be composed the expectation to get what is called the Belmont equation so basically the value of a state is the expected value\", 'chunk36': \"that I just got from this state I'm moved the Queen there which kind of reward did I get 0.2 good and then I can ask mate what could be the cumulative reward in from the next state then so basically this is the finishing because I have the immediate reward plus an estimate of all the future printers can get from this date so I'm using V itself to define me it is a recursive definition the same conditions of consistency in these equation because I'm expecting that I can decompose global expected return into the present reward plus the other expected temperature now\", 'chunk37': \"assuming in these two equations that we know which will be the actions I will take in the future so I'm required for knowledge knowledge of the policy pie of the agent so the policy is specifying which actions I would choose in each state so I'm in a certain state I will take this action because my policy is specifying which action to take so if you put together or this pieces we got the final by my question which is specified the value given a set time policy the value of estate give her something policy as the expectation over the return given the state I'm currently ok and this can be decomposed as a Sam over the rewards that I'm expecting to get\", 'chunk38': \"the future States following the policy that I have at the moment so I think for some of you who may have taken the control system class or maybe they should be quite familiar for the others maybe this is a bit you need to scratch your head a little bit but I think you can have some time to reason about this formula is here the idea is that traditionally in control theory they approached this problem using dynamic programming which is a way to decompose a solution of a combinatorial problem of this kind of a recursive problem by using large tables that I can use to store all the intermediate value calculated so this is totally out of topic for this course but if you're interested you can try to\", 'chunk39': \"Google a little bit about dynamic programming which is very smart techniques for tackling this kind of work your SIM definitions however we have a serious issues with which is that of exponential complexity in the space ok so as we make the space more complicated so if you have learning to play tic-tac-toe you can use dynamic programming are you will learn how to best perform as you will discover the optimal strategy if you're playing chess the state space is too large you cannot use dynamic problem you can do electrical but in practice you have a combinatorial explosion of all the possible actions that you can take and from every action you can go in so many states and you just become crazy in tracking all the possibilities moreover you need to know before the values of the probability of the\", 'chunk40': \"resistance to the next Nexus state given the current state in the action and the reward associated with the state action couple so dynamic programming is great it's very theoretically sound but cannot scale to the award problems so this is why we need Wi-Fi for smart learning and possible so how can you learn an optimal control policy temporal difference learning is one of the most popular ivory so what is the idea we learn and not proximation of the value function by exploiting some prediction of Signals so we start with a very probably at the Beginning is going to be around them estimate of the value of a state I don't know what is the value of a certain state then I choose an action and get somebody word and I decide\", 'chunk41': 'my value was correct or is it needed to be updated so the value I give to a certain state is this expectation over there Sam the discounted Sum Of All The Rewards which can be decomposed into these tutors using the Batman definition the expectation over the current reward plus they discounted value of the next States so the sum of all the possible I can get from that state in the future so after transitioning to the next stage so I choose an action I update my value function to bring it closer to the true about that I upset and the true value is given by the new reward that I get and the better estimate of the value', 'chunk42': \"next Estates so basically if your estimate was correct and you were properly properly estimate in the reward the Delta will be 0 because you your gas about the reward was correct and then your guests about the future rewards from all the other possible stairs doesn't need to be changed however if I was guessing that I was going to receive somebody want from this state and I didn't they need to update the value of this state I was very confident that that States was a very smart move I move my queen I say ok I will get time good reward and I might been got capture or not wait so that stay was not a very good state as I imagined it so I update the value of that state by taking into account that reward that I get\", 'chunk43': \"of course I don't want to make drastic changes the elastic changes to these values I just want to add the data with a little earlier because I want to Ita it over many games and many experiences so this is like a adjusting the weight of a neural network using information about the misclassification using gradients and hear you're not completing the gradient but you're trying to use the new observation to correct your parmiters to correct the value function that you are using to assign the quality of each state so hopefully in the future when the agent has a perfect knowledge of the environment the value function with the perfect and you will always get the reward that you expect\", 'chunk44': \"expectation over the reward is so accurate that you don't need to change your manager's perfectly know which is the value of every state so you can always choose the action that maximizes the value of the max state and the value of the next day and the value of the next to it and these way you maximize your rewards because you're always choosing the best possible action so ideally if you can assign the property value to all the states in your projector at the end you will get the maximum reward in practice you cannot so you estimate the value maybe you're asking mate what's wrong as for you improve it are you continue to do this until you get hopefully to a decent policy where do you get somebody was maybe it's not the optimal policy but it's acceptable there's an even more advice\", 'chunk45': \"what's the way to implement this kind of learning which is called q-learning so it's very similar to temperature difference however we are not just estimating the value of each state but we are estimating the value of each state action is like saying if I'm in this state and I take this action will be when that we are good idea if I mean the same state and I take a different actual would that be a good idea so you also pair the action the possible stage ok so you computer what is colder Q value for the state action pay and you in this case so you also need to condition over all the possible actions so the the real what the cumulative rewards are condition on the current state and accurate action that I chose under this policy so the definitely\", 'chunk46': 'is very similar to the one of the value function but now we are also counting for the action that can be taken so by relating actions to Staithes this new q-learning algorithm allowed to consider which policy is used for predicting the future rewards so we will see for example if I implemented implementing a greedy maxo policy it means that every state I always choose the action corresponding to the maximum q-value so for the same state One action can be a few value or can have a q value of 10 and other actual actual value of one and I will choose the action leading to the highest u-value so why is this powerful because compared to just estimating the value of a state', 'chunk47': \"claiming that state I'm in a dangerous situation because if I choose the wrong action I will lose the game however in that dangerous stay if I choose the correct action I will win the game so maybe overall the value of the state is not very high because it's the Dangerous state it can lead to destruction ok I do lose up to get losing of the game however if you choose the correct action from that risk is there you will win the game so with you learn me you can learn this more fine and drained associations between the state and the action that can be taken every state so this is the way you update your Cuba use in q-learning so basically you have the previous value for the queue of associating one state to every action ok you update this old value with some\", 'chunk48': 'play plus the reward that after taking the action and then the discount over the possible rewards the estimate of future value under some policy which can be the max policy for example I will always choose the action given the highest Fluval u4 the next 8ru contrast this with the previous estimate of your Cuban so this is very similar temperature difference but we are also considering in this case the policy which is allow enough to select the action from a particular state somax is just one possible policy I can choose an action with Samsung stochasticity we will see to explore maybe', 'chunk49': \"so it will not be agreed upon it will be an absolutely because it will allow to some for some randomness in the decision so one property of these approach of dueling is that it is model-free we don't need any explicit explicit knowledge about the environment because we have just estimating Q values so at the beginning my estimate can be around the master or I can have some bounce some upper bounds for example about the human use and then from these gases I guess other gases so this is called bootstrapping in statistics you start from some gases of some quantity you observe by acting using some policy on your initial gases young sir they are more appropriate gas are you up to\", 'chunk50': 'gases and hopefully you will converge to the character values by lot of sampling so in fact Convergence of q-learning is guaranteed So You Win convert to the property values in theory so if you have maybe infinite time and infinite computational power if you have some constraints on the learner right so some very basic properties like the learning about Sam should diverge but the sum of squares should convert so you have to build a serious with some convergence properties over the lemonade but it is quite simple to do in practice so you just have to decide a proper value for the learning rate and theoretically you learning will convert to the optimum to the true cubicles', 'chunk51': \"I saved your reticle but because them in practice may require years or thousands of years so it may not be feasible in practice but this is accurate result and we have seen it all results are important even if not of practical use so cute really was proposed back from the 90s so you can be the original paper in the readings but then was seldomly used in practice because for these limitations you may need a lot of sampling to converge to the proper values of the Q function so how to improve exploration of the environment because if you're using a greedy Max policy like in this case we are a little bit too much constrained over our initial gas\", 'chunk52': \"B&Q values so imagine that you don't know nothing about chats and you start your first chess game you say how I think that moving this piece here and this is there is the best will give me the best value you keep on doing this and you only observed very slowly how to change your reward how to explore other moves are you will always start with the same with Maybe by the move because that is the move with the maximum q-value but that's you value was very imprecise at the Beginning so maybe at the beginning of learning you want to explore the environment by allowing some stochasticity over the selection of the best action so rather than taking the max greedy which is choosing always their maximum Q you can say ok with the probability Epsilon worth\", 'chunk53': \"is usually as more number 0.1 0.2 I choose a random action and then with probability 1 - Epsilon I choose the max value so this is their greedy Max policy but we also include some stochasticity because with some probability we can take around an action and is allowed to explore some other actions that may be with give that to better outcomes that maybe by moving the bishop rather than their the Queen that was another move and saw my random Charles I will discover a better if I stick with the current policy it's going to be very hard to explore better options so this is one example to encourage exploration this is called the Epsilon greedy stochastic policy another\", 'chunk54': \"CBD is to implement of the soft Mac's policy so we already know about softmax the idea is that if you have many possible actions with the max greedy policy you always Tuesday action with the highest Cuban you with the soft Mac's you sample from this distribution so sometimes you sample this action here sometimes you somebody's action here it is very unlikely that you sample this action here because the probability is very small however in some cases you may sample that by chance and you can of course make these choice more or less the past it by using the temperature parameter so at high temperatures or the actions will be almost have the same probability of being sampled so the behaviour would be totally random at high temperature as we and Neil decrease the\", 'chunk55': \"will become more and more materialistic so these are just two options to try to explore the space of actions and make the agent more proficient in solving the task so as I told you usually the explanation profile is starting from my high excellent values if you're doing absolute gravy and then we D Kay Epsilon exponentially or also lenalee so many options exist and by the end of learning as we are experiencing more and more episodes we are sticking with the best action with a max greedy policy if we know that this state with have a particular HiQ value for one action with shoes that killer value for softmax we can change the temperature\", 'chunk56': \"can do some annealing so this is a possibility to improve the trade-off between exploration and exploitation so Explorer at the Beginning exploit by the end of training now and other important distinction is between a policy and a life policy my thoughts and cannot speeding up a bit I understand is my be like a very compressed and condensed plus what is he still also to allow you to explore some terminology because you will see that the terminology for reinforcement learning is very broad and only partially overlap with the terminology for deep learning so I'm trying to sample from all the space of the information related to an enforcement let me to give you a like a broad overview of this app\", 'chunk57': 'we have questions from the rumours sorry I had something not ok so what is the difference between a policy and a life policy methods so let her speak with the q-learning ok so this is the update equation for the queue age you love me agent we are assuming that the update policy is greedy so we are gonna use a Max reports so I update my Q values by taking the current reward so the reward achieved by The Last Action that I performed and then I say in the future I can estimate the rewards that I will get from this date because I know I will behave according to a Max policy in the next stage I will select the maximum value in', 'chunk58': \"plate after I will also select the maximum value so you you select the actions your policy will see like the actions corresponding to the maximum values so you can estimate this quantity because you're assuming how you will sample your actions in the future so the action with the highest expected and you will always be chosen in future States however when I behave I may use absolutely d or I mean you softmax so when I'm planning when I'm estimating the value of a state I may be using a greedy Max policy of deterministic Max policy however when I had to choose the actual I'm a sample it probably so what is kodi update policy because it is used to estimate the future or what\", 'chunk59': \"how long is used to be is called behaviour policy because it is used to select the price of action so if the two policies are the same the I got them is called on policy so I use exactly the same policy for planning for estimating my future Q values and for choosing the action to perform and one some example of these I believe them is the state actually world state action salsa algorithm ok I don't want to go into the details of this method but just realised that this is an important because it uses a stochastic behaviour policy during behaviour and also during estimation of the cubed root during the update function so is it is not using the max policy it is using a stochastic\", 'chunk60': \"IKEA what does it mean we are putting lot of noise in the estimates because for the same estimate one time we are choosing One action in the other another production because it's not deterministic the way we guess it is stochastic so salsa has lots of stochasticity it so sometimes it requires lot of something convert however you learning is cold and cough policy algorithms because it uses a stochastic behaviour policy so when we have to choose the present action we can use absalom greedy or we can use soft matte but I'm during planning when you're we are updating our Q values we use a deterministic Max policy so in this case yeah I got it done is called off-policy because the two policies do do not come\", 'chunk61': \"you may say ok what's the matter of this what is the crucial difference when the difference is that of policy I got rhythms like q-learning usually take more risks so they can exploit more risky behaviour because they assume that in the future they want make mistakes by chance because I'm sampling stochastically reaction so this is the most classical example we are exploring agreed Ward ok so the agent can move up down left right and you have a starting point and you have to reach a certain goal ok so from this point you have to choose down right right up to get to the goal position however you may have some dangerous places like a cliff so if I do down right up\", 'chunk62': \"I will follow from the cliff and they'll be what will be - 10 for example ok so if you are implementing assassin on policy it will randomly explorers it will use the stochastic policy for choosing the action but also for guessing for updating the policy so sometimes from this position it will sample the wrong action so because Santa is a bit more stochastic you it will be more prudent at the end ok it will learn a policy which tolerates some wrong actions it will try to stay away from the cliff however q-learning we say ok from this point here when I have to estimate Mike u-value I will use the max policy so I will never choose this action because I know I know it's a bad action\", 'chunk63': \"I will always Tuesdays action right ok so if you are planning using a greedy policy you can then take more least behavioral and you can walk very close to the cliff and find the accent so q-learning will be more will try to take more risky actions because it will know that we eat we know stochastically choose the wrong then of course if you have asked for castik behaviour policy it may happen that you will fall from the cliff if the accident baby is choosing the wrong action that might happen but it during the planning it will try to be to discover a more fiscal policy now all this theory is great it's interesting it's very much rooted in dynamic programming Ming\", 'chunk64': 'approximations of a value function using calculator mean TD learning soap I got it himself very powerful in practice we have the Curse of dimensionality that we have seen in other occasions the problem is that complex tasks like navigating an environment moving a robotic body or taking the seizures in complicated situations like playing chess playing go planning over a complicated problem in general has a very huge state spaces so as you include a new dimensions in your space it multiplies the the size of the possible spaces that we can visit so if you are saving a value for all the possible stays like in a table you can have huge tables you have to assign', 'chunk65': 'a value function for every possible state and if you have a combinatorial number of stairs in a combinatorial size table so this is not visible in practice so if your agent for example is taken as input 20 by 20 grey scale images and coded by 8-bit every pixel the possible stage is just huge and we cannot assign a different value or even a decent Cuba you for all the states and the corresponding actions because it will require a huge amount of samples for estimating the correct values so this is called the Curse of dimensionality and this is why until 10 years ago even less maybe 6 years ago 67 years ago reinforcement learning was considered to be a very challenging problem so the robots after that', 'chunk66': \"point where a bit at least the Elizabethan cameras on live.ly the modification to the environment like the fact that maybe you lose a little bit of balance you don't have a control policy to counterbalance this or if you look at the earliest RoboCop robot you have a ball that you want to kick and you're trying to put yourself in a good state than the ball moves and you just go crazy and you your policy with totally messed up so this is why we needed deep learning to improve and to make more efficient unfortunately ages so we take a 5-minute break and I we explore the different forcement learning setting of course if you have questions this is a good time to ask them\", 'chunk67': \"I have one so if I understand correctly in this setting we don't need the training face the training is continuous I mean I keep training the model as as long as I keep good question good point so the question is we don't need a training face so it's an incremental process so we start because the the agent is creating its own training experiences by action so you start with a random knowledge on random values or upper bounds over Q values then you choose some actions you play the game you play your experience you received the reward and you update your coronavirus and you keep on doing this for your entire life then if you have to build\", 'chunk68': \"Starbucks or are an agent train using a fortnight let me what you want to do is to train it incrementally and after why you say ok now my agent is ready and you can test it in a new environment or maybe against and you advancery and you playlist for example so yes good point we don't have a separate thingy face and I'm a separate test because the training phase is busy now incrementally but then after a some point you can say ok my agent has converged to a good policy and you can test it over normal scenarios for thank you so you can imagine that one of the most challenging things is that at the Beginning your agent has no knowledge about\", 'chunk69': \"environment no knowledge about the Q values so it needs to sample actions basically you're randomly and try to play a chess game or to learn chess by randomly playing your moves that is going to be very complicated so the way you play Chas usually somebody with experience will tell you you know you have to open the game in a Samsung way you have to move first your course is your body shops later you start moving your queen you try to conquer the centre of the game for somebody's telling you already some high-level strategies if you just randomly play against somebody and you receive a feedback at the end of the game if you lose or win the game that is really messy and really challenging so this is why I'm enforcement Let Me It's Complicated problem\", 'chunk70': \"so what was the key idea that allowed to implement deep reinforcement learning how to combine the power of deep learning with that enforcement learning well to use a neural network to approximate the value function so this was actually proposed back from the 90s it's not a totally new idea so you are trying to estimate some values and you can parameterized this function and in the 90s they were mostly trying to exploit linear models to estimate the value function but also somebody was proposing to use your Netflix so I think one of the first backgammon agents at well able to\", 'chunk71': \"play backgammon was based on an artificial neural networks train to approximate the value function so what is the advantage here we don't need a tabular of table with all the values stored for every possible state which can be coming up really large we approximately is using a function parameterized to the according to their connection with and this way you are proximate a table with function and you can allow for generalization for similar stay we got a similar value you don't need to store two separate value for two different states if the stage share my information ok so these allowed to improve the generalization and make the representation of the problem much more efficient and simple more recently\", 'chunk72': \"people are also starting to use deep learning not just to approximate the value function or the cube values but also to directly le the policy so you also get as the output of the neural network the action that needs to be performed so you select maybe the highest action using soft marks or you sample from the actions possible actions that we changed environment which will give the reward and a new observation as input and you really playing the entire system using back-propagation based on the reward that you get and the actions that you're something from your new amato it is a little bit more complicated because we have directory trying to learn the policy and policy gradient methods I don't think we will have time to dig into this that my phone's\", 'chunk73': \"maybe next class but let's imagine for the moment that we can use a neural network to either approximate value function or the queue function ok so that your values for every state action pair or even the policy so the way we sampled the action giving acceptance date so what are the issues of curing when we start using neural networks to approximate Q values for example as we just said the training set is being created incremental so at the beginning the values are totally random and we'll leave find them with her kwasa kwasa and so he needs a lot of time for the age and to have some to bootstrap some good knowledge from the environment so what happens to a new romantic when you have a random value as input and almost random values output it's not really converging is very unstable ok\", 'chunk74': \"so it's not as we are trying to map input images to classes on input images to reconstruction of the input images like in supervised or unsupervised learning we are trying to estimate some values that constantly being updated and changes depending on the actions that will take is very hard to have a neural network learning with some stability this kind of distribution over the training points are highly correlated so the actual time taking Wilko some new state and we'll cause new actions to be taken so the entire episode is highly correlated if you're playing a chess game the actual that have taken so far are very important to define the new state and the actual that you will take later neural networks usually wants independent and identically distributed samples so this is not the case and the target thanks\", 'chunk75': \"Johnny's non-stationary because it changes as we learn as we learn to interact with the environment you are playing chess at the Beginning you have totally now if you do random owes you always lose the game your your advancery your opponent with play my easy then as you get better your opponent with changing the strategy and start to play more seriously and then as you stop beating some opponents you go playing against champions and so they're strategy with the game be different so the target function is always changing and becoming more and more complicated you are metals have not bored to deal with this time of nonstationarity so they die of using neural networks to approximate the value function of Cuba or Cuba or the policy is interesting but I need practice has this complication so the first walk around and\", 'chunk76': \"people introduced to implement to learning using neural networks used to maintain two separate networks which is called the prediction network which is just trying to estimate the cubed use given the state in the action and the current parameters the feet that the connection weights and this prediction I thought is train at each step so everytime I observed the reward I update this part however we have a target network which is basically implementing the policy that we used to choose which actions to take this call the target network and we don't update this network at every time point we updated after and episodes orsi isolations ok so we have a hyperparameter called The Sea which\", 'chunk77': \"is telling us how frequently to change the target so destabilising learning a little bit because the target network will behave consistently in the same way for CIA operations and their prediction that we learn something by The Rewards that we obtain and then update or should I tell you that it's not all we behave consistently for a series of episodes and prediction network with to buy the estimate to buy the product that you value but at least the policy is fix it because the target network is not changed and then after a while we update the policy so basically this way we are making the system more stable because we are not changing the way to every time we have changing the way from the target network after some episodes of learning so this is the first work around that has been introduced to use\", 'chunk78': \"neural networks in Portsmouth learning settings II very important work around is called experience replay and again I was kind of surprised to realise it was first introduced back in 1993 so this is a very good idea but then you have to implement it may be using some cakes and some more efficient mechanisms I don't know why it was not working like in the time I should learn something about this day that we are using a memory buffer so with simple have we have a simple memory system which is storing previous learning at results which means state action reward Nexus state capitals ok so saoars is representing a particular state in the system where we have choosing a particular action and we have received\", 'chunk79': \"and we have jam in a particular new state we save this information in one line in the buffer then will explain how our environment we are following our policy we are getting our rewards and we see another state and other action and new state we say most of this in the bath ok so we save all the this state actually was State capitals and during learning rather than simply following the correct order wee sample randomly from this buffer so I'm acting but I'm also sampling an action that I've taken maybe 10 minutes ago and hideous away to angry correlate the examples and to facilitate the neural network learning because the neural network is expecting uncorrelated samples\", 'chunk80': \"what if you sometimes sample from this buffer some painting episodes tiny experiences you can make the training samples less correlated and you will stabilize learning and of course the way you sample from this replay buffer can be totally random or can be prioritised so for example the states that gave some highest rewards may be more interesting to repeat so you can sample only the tappos with the same level of reward because the doors may have been the most interesting actions that were taking so of course you can complicate the way you replay the experience by using prioritise the replay but in general the idea is that we maintain a basket of memories that have sampled offline so it's not there\", 'chunk81': \"he's always learning by random sampling episodes as the agent becomes more and more proficient de Replay buffer will be composed by the best actions and the best states in the most promising States but I'm usually it will be composed by totally random so this is probably the first example of a successful use of deep learning in combination with enforcement learning so this is called the Deep Q network published by Google deepmind in 2015 and this was really make a lot of rumours in the news because this system managed to outperform humans in manage challenging at again so Atari games are cannot games from like the idea of PlayStation\", 'chunk82': 'a pizza gas 80s 90s would you have quite complicated inputs because you have an entire screen of pixels you have an agent which is trying to sort some simple games in the screen ok so you have a deep convolutional network trying to extract some high-level representations from the pixels in your screen and then you have a controller which is telling what are the actions that needs to be before like moving the agent up down left right shooting you may have different actions depending on the game and of course you have a new state of served after you take the action and somebody Ward which is coming to the agent in fact the input to the system was composed by the past 4 frames to give a little bit of temporal context to the agent ok but the output was there direct', 'chunk83': \"reaction probabilities and then you have a guess a maximum policy max-q policy for selecting the action policy if I'm not wrong so what is impressive is that this system is able to discover strategies for playing challenging games so in this simple demonstration you can see how the agent is improving overtime so at the Beginning after 10 minutes of playing the I got ISM is quite around so you'll see the only actions allowed at left or right in this case so this is quite simple game is called the breakout game so for those of you who are not familiar with this basically this is the agent which is moving last alright and the goal is to have the ball crashing all the\", 'chunk84': 'coloured lines and the final goal would be to destroy as much call of the pixels as possible and see you have the current score one because we only destroyed one and you have the lives of the AJ every time the body is going below the level of the agent you will lose 1 life and you start with 5 lives and that you immediately go to zero because the agent is losing most of their balls after 2-hours you can see the level achieved by this agent it is playing like an expert between almost never miss the ball it was get more and more rewards and it was lovely love to destroy all the Call of the squares what is even more interesting and surprising maybe it at some point after lot of', 'chunk85': 'gaming the agent with discoverer strategy which is to destroy as much possible in one side so try to f*** you the bouncing in order to have the game sold quite easily so this is I think quite amazing because you can see how I am a giant by just trial and error can discover sophisticated planning strategies sophisticated policies ok maybe we will start the next classic somebody so', 'chunk86': 'if you have questions we have some time to discuss them otherwise we will finish with alphago in the next class and then we go into some more advanced topics related to Deep reinforcement Learning please regarding the symbol in the incubator tried to use recurrent neural network in order to exploit that information instead of buffering the data and assembling so good Passion advanced ways to tackle this one possibility is to use a recurring to learn a forward model of the environment that can actually exploit to the correlation in the data to predict the next stay'}\n"
     ]
    }
   ],
   "source": [
    "print(diz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_chunks=[diz['chunk{}'.format(i+1)] for i in range(len(diz))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "text='\\n'.join(l_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'so as I was saying today we wi'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ready!\n"
     ]
    }
   ],
   "source": [
    "with open('recognized.txt',mode ='w') as file: \n",
    "   file.write(\"Recognized Speech:\") \n",
    "   file.write(\"\\n\") \n",
    "   file.write(text) \n",
    "   print(\"ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
